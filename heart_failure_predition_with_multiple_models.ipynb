{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqFLBeX-H9yM",
        "outputId": "4bd915e8-c207-4862-fd6c-138a764dd9c2"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "positional indexers are out-of-bounds",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Abhi dobariya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Abhi dobariya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4154\u001b[0m \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Abhi dobariya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Abhi dobariya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:891\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    890\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m--> 891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
            "File \u001b[1;32mc:\\Users\\Abhi dobariya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:282\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n, verify)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
            "\u001b[1;31mIndexError\u001b[0m: indices are out-of-bounds",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Calculate the accuracy score\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#accuracy = accuracy_score(y_test, y_pred)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m idx \u001b[38;5;241m=\u001b[39m temp[temp\u001b[38;5;241m.\u001b[39mmax_prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.9\u001b[39m]\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m---> 38\u001b[0m accuracy2 \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43my_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, y_rf[idx])\n\u001b[0;32m     41\u001b[0m accuracy2\n",
            "File \u001b[1;32mc:\\Users\\Abhi dobariya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Abhi dobariya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1743\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1747\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
            "File \u001b[1;32mc:\\Users\\Abhi dobariya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1717\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1716\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[1;32m-> 1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
            "\u001b[1;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = data.drop(columns=['DEATH_EVENT'])\n",
        "y = data['DEATH_EVENT']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Fit the Logistic Regression model\n",
        "#log_reg = LogisticRegression(random_state=42)\n",
        "model = RandomForestClassifier(random_state=43)\n",
        "model.fit(X_train,y_train)\n",
        "temp = pd.DataFrame(model.predict_proba(X_train).tolist(), columns=model.classes_)\n",
        "temp['max_prob'] = temp.max(axis=1)\n",
        "\n",
        "#log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "#y_pred = log_reg.predict(X_test_scaled)\n",
        "y_rf = model.predict(X_test)\n",
        "# Calculate the accuracy score\n",
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "idx = temp[temp.max_prob >= .9].index\n",
        "accuracy2 = accuracy_score(y_test.iloc[idx], y_rf[idx])\n",
        "\n",
        "\n",
        "accuracy2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1.0, 0.2833333333333333)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = data.drop(columns=['DEATH_EVENT'])\n",
        "y = data['DEATH_EVENT']\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fit the Random Forest Classifier model\n",
        "model = RandomForestClassifier(random_state=43)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "probs = model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Convert the predicted probabilities to a DataFrame\n",
        "probs_df = pd.DataFrame(probs, columns=model.classes_)\n",
        "\n",
        "# Add a column for the maximum predicted probability\n",
        "probs_df['max_prob'] = probs_df.max(axis=1)\n",
        "\n",
        "# Get the predicted class using the maximum probability\n",
        "probs_df['pred_class'] = model.predict(X_test_scaled)\n",
        "\n",
        "# Set a probability threshold (e.g., 0.9 for high-confidence predictions)\n",
        "threshold = 0.9275\n",
        "\n",
        "# Filter rows where the maximum probability is above the threshold\n",
        "high_confidence_idx = probs_df[probs_df['max_prob'] >= threshold].index\n",
        "data2 = pd.DataFrame(data)\n",
        "newData = data2.loc[high_confidence_idx]\n",
        "newData = data2.drop(data2.index.difference(high_confidence_idx))\n",
        "\n",
        "# Get the high-confidence predictions and their corresponding true labels\n",
        "high_confidence_preds = probs_df.loc[high_confidence_idx, 'pred_class']\n",
        "high_confidence_true = y_test.iloc[high_confidence_idx]\n",
        "\n",
        "# Calculate the accuracy for high-confidence predictions\n",
        "accuracy_high_confidence = accuracy_score(high_confidence_true, high_confidence_preds)\n",
        "\n",
        "\n",
        "accuracy_high_confidence , len(high_confidence_idx)/len(y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qcLeJXiKDKD"
      },
      "source": [
        "#Heart Failure Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3OC1UVGKITO"
      },
      "source": [
        "##Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "collapsed": true,
        "id": "0JqnqzzXJVae",
        "outputId": "d0510a8a-df8e-4bce-e3ea-0ba5b617244e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(age                         0\n",
              " anaemia                     0\n",
              " creatinine_phosphokinase    0\n",
              " diabetes                    0\n",
              " ejection_fraction           0\n",
              " high_blood_pressure         0\n",
              " platelets                   0\n",
              " serum_creatinine            0\n",
              " serum_sodium                0\n",
              " sex                         0\n",
              " smoking                     0\n",
              " time                        0\n",
              " DEATH_EVENT                 0\n",
              " dtype: int64,\n",
              "               age     anaemia  creatinine_phosphokinase    diabetes  \\\n",
              " count  299.000000  299.000000                299.000000  299.000000   \n",
              " mean    60.833893    0.431438                581.839465    0.418060   \n",
              " std     11.894809    0.496107                970.287881    0.494067   \n",
              " min     40.000000    0.000000                 23.000000    0.000000   \n",
              " 25%     51.000000    0.000000                116.500000    0.000000   \n",
              " 50%     60.000000    0.000000                250.000000    0.000000   \n",
              " 75%     70.000000    1.000000                582.000000    1.000000   \n",
              " max     95.000000    1.000000               7861.000000    1.000000   \n",
              " \n",
              "        ejection_fraction  high_blood_pressure      platelets  \\\n",
              " count         299.000000           299.000000     299.000000   \n",
              " mean           38.083612             0.351171  263358.029264   \n",
              " std            11.834841             0.478136   97804.236869   \n",
              " min            14.000000             0.000000   25100.000000   \n",
              " 25%            30.000000             0.000000  212500.000000   \n",
              " 50%            38.000000             0.000000  262000.000000   \n",
              " 75%            45.000000             1.000000  303500.000000   \n",
              " max            80.000000             1.000000  850000.000000   \n",
              " \n",
              "        serum_creatinine  serum_sodium         sex    smoking        time  \\\n",
              " count         299.00000    299.000000  299.000000  299.00000  299.000000   \n",
              " mean            1.39388    136.625418    0.648829    0.32107  130.260870   \n",
              " std             1.03451      4.412477    0.478136    0.46767   77.614208   \n",
              " min             0.50000    113.000000    0.000000    0.00000    4.000000   \n",
              " 25%             0.90000    134.000000    0.000000    0.00000   73.000000   \n",
              " 50%             1.10000    137.000000    1.000000    0.00000  115.000000   \n",
              " 75%             1.40000    140.000000    1.000000    1.00000  203.000000   \n",
              " max             9.40000    148.000000    1.000000    1.00000  285.000000   \n",
              " \n",
              "        DEATH_EVENT  \n",
              " count    299.00000  \n",
              " mean       0.32107  \n",
              " std        0.46767  \n",
              " min        0.00000  \n",
              " 25%        0.00000  \n",
              " 50%        0.00000  \n",
              " 75%        1.00000  \n",
              " max        1.00000  )"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Load the data\n",
        "data = pd.read_csv('heart_failure_clinical_records_dataset.csv')\n",
        "data\n",
        "\n",
        "data.dropna()\n",
        "\n",
        "data.isnull().sum() , data.describe()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7JFJXJFKNQl"
      },
      "source": [
        "##Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "HGNJszQeKQGG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8525\n",
            "Precision: 0.8491\n",
            "Recall: 0.7031\n",
            "F1-score: 0.7692\n",
            "ROC AUC: 0.8179\n"
          ]
        }
      ],
      "source": [
        "# Separate features and target \n",
        "X = data.drop(columns=['DEATH_EVENT'])\n",
        "y = data['DEATH_EVENT']\n",
        "\n",
        "# Split the data into training and testing sets \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.61, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled,y_train)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "# time (0.375) positive \n",
        "# serum_creatinine (0.155)\n",
        "# ejection_fraction (0.112)\n",
        "# creatinine_phosphokinase (0.083)\n",
        "# age (0.078)\n",
        "# platelets (0.075)\n",
        "\n",
        "\n",
        "# Evaluate model performance with multiple metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yybRwd-bLMum"
      },
      "source": [
        "##Saving The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEBXTpllJwne",
        "outputId": "3da3bba0-7688-4c91-bffa-1400e270d5ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['heart_failure_model.pkl']"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, 'heart_failure_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SALIJxIULPw4"
      },
      "source": [
        "##Loading The model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IwUO0OzJ37l"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "loaded_model = joblib.load('heart_failure_model.pkl')\n",
        "\n",
        "# Prepare new data for prediction (ensure it has the same features as the training data)\n",
        "new_data = ...  # Your new data\n",
        "\n",
        "# Make predictions\n",
        "predictions = loaded_model.predict(new_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 727551,
          "sourceId": 1263738,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30761,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
